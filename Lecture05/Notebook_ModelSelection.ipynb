{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5653c385",
   "metadata": {},
   "source": [
    "<div >\n",
    "<img src = \"../banner.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0a685-384c-4c7e-9905-1243108b4592",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/BDML_202402/blob/main/Lecture05/Notebook_ModelSelection.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e57de4",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44efaa5f-efe9-424e-bf07-c46550bc67fb",
   "metadata": {},
   "source": [
    "The process of model selection involves choosing the most appropriate model from a set of potential candidates based on their predictive performance. Model selection techniques are essential to identify the model that best balances complexity and accuracy without overfitting or underfitting the data.\n",
    "\n",
    "Among the various strategies for model selection, three popular methods are Best Subset Selection, Forward Selection, and Backward Selection. Each method has a distinct approach to selecting variables.\n",
    "\n",
    "In the following sections, we will explore each of these methods in detail, highlighting their procedures, advantages, and when to use them.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a3dcd",
   "metadata": {},
   "source": [
    "# Running Example: Predicting Wages\n",
    "\n",
    "Our objective today is to construct a model of individual wages\n",
    "\n",
    "$$\n",
    "w = f(X) + u \n",
    "$$\n",
    "\n",
    "where w is the  wage, and X is a matrix that includes potential explanatory variables/predictors. In this problem set, we will focus on a linear model of the form\n",
    "\n",
    "\\begin{align}\n",
    " ln(w) & = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p  + u \n",
    "\\end{align}\n",
    "\n",
    "were $ln(w)$ is the logarithm of the wage.\n",
    "\n",
    "To illustrate I'm going to use a sample of the NLSY97. The NLSY97 is  a nationally representative sample of 8,984 men and women born during the years 1980 through 1984 and living in the United States at the time of the initial survey in 1997.  Participants were ages 12 to 16 as of December 31, 1996.  Interviews were conducted annually from 1997 to 2011 and biennially since then.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab5a6",
   "metadata": {},
   "source": [
    "Let's load the packages and the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83615f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"pacman\") #for google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516fd2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "require(\"pacman\")\n",
    "p_load(\"tidyverse\")\n",
    "\n",
    "nlsy <- read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/nlsy97.csv')\n",
    "\n",
    "nlsy <- nlsy  %>%   drop_na(educ) #dropea los valores faltantes (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b2617-7f7f-4d39-b431-8d07e8f73044",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(nlsy[1:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table(nlsy$yhea_100_1997)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038f3fa",
   "metadata": {},
   "source": [
    "Let's keep a subset of these predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlsy<- nlsy[1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888edb7-99df-41b9-bb50-624917d795f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(nlsy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d13f2f",
   "metadata": {},
   "source": [
    "### Best Subset Selection\n",
    "\n",
    "\n",
    "1.  Let $M_0$ denote the null model, which contains no predictors. This\n",
    "    model simply predicts the sample mean for each observation.\n",
    "\n",
    "2.  For $k=1,2,\\dots,p$:\n",
    "\n",
    "    1.  Fit all $\\binom{p}{k}$ models that contain exactly k predictors\n",
    "\n",
    "    2.  Pick the best among these $\\binom{p}{k}$ models, and call it\n",
    "        $M_k$. Where *best* is the one with the smallest $MSE$\n",
    "\n",
    "3.  Select a single best model from among $M_0,\\dots, M_p$, using cross-validated prediction error, Cp (AIC), BIC, or adjusted R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_load(\"leaps\")\n",
    "\n",
    "#performs step 2\n",
    "subset<-regsubsets(lnw_2016 ~ ., method=\"exhaustive\",nvmax=14,data = nlsy)\n",
    "\n",
    "summary(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b684b-838f-4f25-97ab-edc70435a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "best_subset <- summary(subset)\n",
    "\n",
    "\n",
    "data.frame(\n",
    "  Adj.R2 = which.max(best_subset$adjr2),\n",
    "  CP = which.min(best_subset$cp),\n",
    "  BIC = which.min(best_subset$bic)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a504b-8c87-4d2c-bed4-e39329f011c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(subset,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1318f0-248e-41cf-b5c6-3a9fec9f9d99",
   "metadata": {},
   "source": [
    "##  Stepwise Selection\n",
    "\n",
    "-   For computational reasons, best subset selection cannot be applied\n",
    "    with very large p.\n",
    "\n",
    "-   Best subset selection may also suffer from statistical problems when\n",
    "    p is large\n",
    "\n",
    "-   An enormous search space can lead to overfitting and high variance\n",
    "    of the coefficient estimates.\n",
    "\n",
    "-   For both of these reasons, stepwise methods, which explore a far\n",
    "    more restricted set of models, are attractive alternatives to best\n",
    "    subset selection.\n",
    "\n",
    "\n",
    "\n",
    "###  Forward Stepwise Selection\n",
    "\n",
    "    -   Start with no predictors\n",
    "\n",
    "    -   Test all models with 1 predictor. Choose the best model\n",
    "\n",
    "    -   Add 1 predictor at a time, without taking away.\n",
    "\n",
    "    -   Of the p+1 models, choose the one with smallest prediction error\n",
    "        using cross validation\n",
    "        \n",
    "    -   We have $1+ p(p+1)/2$ Models. In best subset we had $2^p$ \n",
    "\n",
    "### Backward Stepwise Selection\n",
    "\n",
    "    -   Same idea but start with a complete model and go backwards,\n",
    "        taking one at a time.\n",
    "\n",
    "\n",
    "### Forward Selection\n",
    "\n",
    "-   Computational advantage over best subset selection is clear.\n",
    "\n",
    "-   It is not guaranteed to find the best possible model out of all\n",
    "    $2^p$ models containing subsets of the p predictors.\n",
    "\n",
    "-   Drawback: once a predictor enters, it cannot leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58092a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward<-regsubsets(lnw_2016 ~ ., method=\"forward\", nvmax=14,data = nlsy)\n",
    "\n",
    "summary(forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d14d89-a450-4d46-9319-e3f707fbdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "best_forward <- summary(forward)\n",
    "\n",
    "\n",
    "data.frame(\n",
    "  Adj.R2 = which.max(best_forward$adjr2),\n",
    "  CP = which.min(best_forward$cp),\n",
    "  BIC = which.min(best_forward$bic)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3971bb-51ab-4b8d-b5b2-5982e71d951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(forward, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ce052-dc81-4c80-8feb-c876025bcc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(subset,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42ed3a",
   "metadata": {},
   "source": [
    "### Backward Selection\n",
    "\n",
    "-   Like forward stepwise selection, the backward selection approach\n",
    "    searches through only $1 + p(p + 1)/2$ models\n",
    "\n",
    "-   However, unlike forward stepwise selection, it begins with the model\n",
    "    containing all p predictors, and then iteratively removes the least\n",
    "    useful predictor, one-at-a-time.\n",
    "\n",
    "-   Like forward stepwise selection, backward stepwise selection is not\n",
    "    guaranteed to yield the best model containing a subset of the p\n",
    "    predictors.\n",
    "\n",
    "-   Backward selection requires that the number of observations\n",
    "    (samples) $n$ is larger than the number of variables $p$ (so that\n",
    "    the full model can be fit).\n",
    "\n",
    "-   In contrast, forward stepwise can be used even when $n < p$, and so\n",
    "    is the only viable subset method when p is very large.\n",
    "    \n",
    "Not much has to change to implement backward selection... just looping through the predictors in reverse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "backwards<-regsubsets(lnw_2016 ~ ., method=\"backward\", nvmax=14,data = nlsy)\n",
    "\n",
    "summary(backwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fab4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "best_backwards <- summary(backwards)\n",
    "\n",
    "\n",
    "data.frame(\n",
    "  Adj.R2 = which.max(best_backwards$adjr2),\n",
    "  CP = which.min(best_backwards$cp),\n",
    "  BIC = which.min(best_backwards$bic)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae36f1-81a3-4498-a037-dc30aa0c829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(backwards, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d94319-3e7f-47dd-8d9e-ccc978655e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(forward, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f191a8b-4b61-4d8c-89cd-3d3b7835f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(subset,6)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
